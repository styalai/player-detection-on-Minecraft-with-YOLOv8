{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54387c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python\n",
    "#!pip install opencv-contrib-python\n",
    "#!pip install ultralytics\n",
    "#!pip install openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba957f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 352x640 (no detections), 79.4ms\n",
      "Speed: 5.3ms preprocess, 79.4ms inference, 6.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "2.6009687503294385\n",
      "\n",
      "0: 352x640 (no detections), 85.7ms\n",
      "Speed: 2.7ms preprocess, 85.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "3.8260591938035833\n",
      "\n",
      "0: 352x640 (no detections), 55.3ms\n",
      "Speed: 1.4ms preprocess, 55.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "4.836564266446087\n",
      "\n",
      "0: 352x640 (no detections), 65.7ms\n",
      "Speed: 1.2ms preprocess, 65.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "5.4920239502453985\n",
      "\n",
      "0: 352x640 (no detections), 53.0ms\n",
      "Speed: 1.1ms preprocess, 53.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "6.080631388803113\n",
      "\n",
      "0: 352x640 (no detections), 62.8ms\n",
      "Speed: 1.2ms preprocess, 62.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "6.474809659950478\n",
      "\n",
      "0: 352x640 (no detections), 57.2ms\n",
      "Speed: 1.3ms preprocess, 57.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "6.814616546030376\n",
      "\n",
      "0: 352x640 (no detections), 51.1ms\n",
      "Speed: 1.2ms preprocess, 51.1ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "7.126740245407842\n",
      "\n",
      "0: 352x640 (no detections), 49.0ms\n",
      "Speed: 1.3ms preprocess, 49.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "7.328829883523035\n",
      "\n",
      "0: 352x640 (no detections), 56.9ms\n",
      "Speed: 1.5ms preprocess, 56.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "7.543371693112415\n",
      "\n",
      "0: 352x640 (no detections), 58.0ms\n",
      "Speed: 1.4ms preprocess, 58.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "7.725423901366075\n",
      "\n",
      "0: 352x640 (no detections), 55.6ms\n",
      "Speed: 1.0ms preprocess, 55.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "7.8973085385107105\n",
      "\n",
      "0: 352x640 (no detections), 50.0ms\n",
      "Speed: 1.2ms preprocess, 50.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "8.080285440639429\n",
      "\n",
      "0: 352x640 (no detections), 51.4ms\n",
      "Speed: 1.1ms preprocess, 51.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "8.234020153069094\n",
      "\n",
      "0: 352x640 1 player, 52.6ms\n",
      "Speed: 1.2ms preprocess, 52.6ms inference, 4.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "2  [[223, 0, 61, 113]]\n",
      "8.324818798462818\n",
      "\n",
      "0: 352x640 (no detections), 48.7ms\n",
      "Speed: 1.1ms preprocess, 48.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "8.462567667955051\n",
      "\n",
      "0: 352x640 1 player, 51.3ms\n",
      "Speed: 1.4ms preprocess, 51.3ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "2  [[225, 0, 71, 114]]\n",
      "8.573941306584146\n",
      "\n",
      "0: 352x640 (no detections), 48.1ms\n",
      "Speed: 1.2ms preprocess, 48.1ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "8.691167831609949\n",
      "\n",
      "0: 352x640 (no detections), 49.9ms\n",
      "Speed: 1.5ms preprocess, 49.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "8.793573342107953\n",
      "\n",
      "0: 352x640 (no detections), 46.9ms\n",
      "Speed: 1.2ms preprocess, 46.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "8.898134772998388\n",
      "\n",
      "0: 352x640 (no detections), 49.2ms\n",
      "Speed: 1.2ms preprocess, 49.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "8.989391278593473\n",
      "\n",
      "0: 352x640 (no detections), 51.9ms\n",
      "Speed: 1.1ms preprocess, 51.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.063841089578066\n",
      "\n",
      "0: 352x640 (no detections), 50.6ms\n",
      "Speed: 1.2ms preprocess, 50.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.135086779496474\n",
      "\n",
      "0: 352x640 (no detections), 49.0ms\n",
      "Speed: 1.0ms preprocess, 49.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.206096310393137\n",
      "\n",
      "0: 352x640 (no detections), 46.7ms\n",
      "Speed: 1.5ms preprocess, 46.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.281342463706695\n",
      "\n",
      "0: 352x640 (no detections), 46.4ms\n",
      "Speed: 1.0ms preprocess, 46.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.353177447477101\n",
      "\n",
      "0: 352x640 (no detections), 60.6ms\n",
      "Speed: 1.2ms preprocess, 60.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.373109423977688\n",
      "\n",
      "0: 352x640 (no detections), 46.8ms\n",
      "Speed: 1.5ms preprocess, 46.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.434565739532891\n",
      "\n",
      "0: 352x640 (no detections), 48.2ms\n",
      "Speed: 1.1ms preprocess, 48.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.490032271698002\n",
      "\n",
      "0: 352x640 (no detections), 45.8ms\n",
      "Speed: 1.2ms preprocess, 45.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.551314326365752\n",
      "\n",
      "0: 352x640 (no detections), 47.7ms\n",
      "Speed: 1.1ms preprocess, 47.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.602053583099295\n",
      "\n",
      "0: 352x640 (no detections), 45.6ms\n",
      "Speed: 1.0ms preprocess, 45.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.655166043683783\n",
      "\n",
      "0: 352x640 (no detections), 52.5ms\n",
      "Speed: 1.1ms preprocess, 52.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.6858835899986\n",
      "\n",
      "0: 352x640 (no detections), 43.3ms\n",
      "Speed: 1.1ms preprocess, 43.3ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.742614166101731\n",
      "\n",
      "0: 352x640 (no detections), 51.7ms\n",
      "Speed: 1.1ms preprocess, 51.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.772035644609387\n",
      "\n",
      "0: 352x640 (no detections), 46.0ms\n",
      "Speed: 1.1ms preprocess, 46.0ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.81457017178153\n",
      "\n",
      "0: 352x640 (no detections), 46.2ms\n",
      "Speed: 1.0ms preprocess, 46.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.8646828974403\n",
      "\n",
      "0: 352x640 (no detections), 48.8ms\n",
      "Speed: 1.0ms preprocess, 48.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.898968059615079\n",
      "\n",
      "0: 352x640 (no detections), 52.0ms\n",
      "Speed: 1.0ms preprocess, 52.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.924636932692449\n",
      "\n",
      "0: 352x640 (no detections), 51.0ms\n",
      "Speed: 1.1ms preprocess, 51.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.948343047326155\n",
      "\n",
      "0: 352x640 (no detections), 56.2ms\n",
      "Speed: 1.2ms preprocess, 56.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.95818507027912\n",
      "\n",
      "0: 352x640 (no detections), 60.9ms\n",
      "Speed: 1.4ms preprocess, 60.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.956993279228405\n",
      "\n",
      "0: 352x640 (no detections), 56.6ms\n",
      "Speed: 1.4ms preprocess, 56.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.968933949917728\n",
      "\n",
      "0: 352x640 (no detections), 57.7ms\n",
      "Speed: 2.8ms preprocess, 57.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.960209005883586\n",
      "\n",
      "0: 352x640 (no detections), 45.6ms\n",
      "Speed: 1.0ms preprocess, 45.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.993826123735493\n",
      "\n",
      "0: 352x640 (no detections), 49.9ms\n",
      "Speed: 1.2ms preprocess, 49.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.015238645375765\n",
      "\n",
      "0: 352x640 (no detections), 46.6ms\n",
      "Speed: 1.0ms preprocess, 46.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.042773271015506\n",
      "\n",
      "0: 352x640 (no detections), 46.9ms\n",
      "Speed: 1.2ms preprocess, 46.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.069424941226933\n",
      "\n",
      "0: 352x640 (no detections), 52.9ms\n",
      "Speed: 1.3ms preprocess, 52.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.082352227244789\n",
      "\n",
      "0: 352x640 (no detections), 49.5ms\n",
      "Speed: 1.2ms preprocess, 49.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.102038609341536\n",
      "\n",
      "0: 352x640 (no detections), 55.1ms\n",
      "Speed: 1.5ms preprocess, 55.1ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.109100848899901\n",
      "\n",
      "0: 352x640 (no detections), 51.6ms\n",
      "Speed: 1.5ms preprocess, 51.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.124168977367702\n",
      "\n",
      "0: 352x640 (no detections), 49.8ms\n",
      "Speed: 1.1ms preprocess, 49.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.140780686169458\n",
      "\n",
      "0: 352x640 (no detections), 60.6ms\n",
      "Speed: 1.7ms preprocess, 60.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.13616053817162\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 352x640 (no detections), 51.3ms\n",
      "Speed: 1.6ms preprocess, 51.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.14846068639658\n",
      "\n",
      "0: 352x640 (no detections), 68.4ms\n",
      "Speed: 1.1ms preprocess, 68.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.130180270964752\n",
      "\n",
      "0: 352x640 (no detections), 69.4ms\n",
      "Speed: 1.2ms preprocess, 69.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.111451366322397\n",
      "\n",
      "0: 352x640 (no detections), 60.3ms\n",
      "Speed: 1.1ms preprocess, 60.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.107622094272385\n",
      "\n",
      "0: 352x640 (no detections), 56.2ms\n",
      "Speed: 1.3ms preprocess, 56.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.112413961463922\n",
      "\n",
      "0: 352x640 (no detections), 55.4ms\n",
      "Speed: 1.4ms preprocess, 55.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.118257305954744\n",
      "\n",
      "0: 352x640 (no detections), 58.9ms\n",
      "Speed: 1.2ms preprocess, 58.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.11784168829646\n",
      "\n",
      "0: 352x640 (no detections), 67.3ms\n",
      "Speed: 1.4ms preprocess, 67.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.104093355571804\n",
      "\n",
      "0: 352x640 (no detections), 65.9ms\n",
      "Speed: 1.2ms preprocess, 65.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.093869906918224\n",
      "\n",
      "0: 352x640 (no detections), 58.9ms\n",
      "Speed: 1.2ms preprocess, 58.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.093360123077607\n",
      "\n",
      "0: 352x640 (no detections), 50.2ms\n",
      "Speed: 1.4ms preprocess, 50.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.106694056796757\n",
      "\n",
      "0: 352x640 (no detections), 46.7ms\n",
      "Speed: 1.1ms preprocess, 46.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.124283576280998\n",
      "\n",
      "0: 352x640 (no detections), 50.8ms\n",
      "Speed: 1.2ms preprocess, 50.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.135977989413105\n",
      "\n",
      "0: 352x640 (no detections), 53.7ms\n",
      "Speed: 1.2ms preprocess, 53.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.143052724780528\n",
      "\n",
      "0: 352x640 (no detections), 62.4ms\n",
      "Speed: 1.2ms preprocess, 62.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.137285308138548\n",
      "\n",
      "0: 352x640 (no detections), 50.8ms\n",
      "Speed: 1.1ms preprocess, 50.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.15052232437927\n",
      "\n",
      "0: 352x640 (no detections), 51.6ms\n",
      "Speed: 1.9ms preprocess, 51.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.15980241518256\n",
      "\n",
      "0: 352x640 (no detections), 61.5ms\n",
      "Speed: 1.0ms preprocess, 61.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.155754061235395\n",
      "\n",
      "0: 352x640 (no detections), 50.9ms\n",
      "Speed: 1.1ms preprocess, 50.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.166764194500175\n",
      "\n",
      "0: 352x640 (no detections), 53.4ms\n",
      "Speed: 1.3ms preprocess, 53.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.173492217755351\n",
      "\n",
      "0: 352x640 (no detections), 215.2ms\n",
      "Speed: 1.5ms preprocess, 215.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.961044388966123\n",
      "\n",
      "0: 352x640 (no detections), 64.2ms\n",
      "Speed: 1.1ms preprocess, 64.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.957852009611631\n",
      "\n",
      "0: 352x640 (no detections), 43.9ms\n",
      "Speed: 1.8ms preprocess, 43.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.977775571900672\n",
      "\n",
      "0: 352x640 (no detections), 50.5ms\n",
      "Speed: 1.0ms preprocess, 50.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.990653626528678\n",
      "\n",
      "0: 352x640 (no detections), 55.6ms\n",
      "Speed: 1.1ms preprocess, 55.6ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.995227750310384\n",
      "\n",
      "0: 352x640 (no detections), 63.1ms\n",
      "Speed: 1.4ms preprocess, 63.1ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "9.992141050265504\n",
      "\n",
      "0: 352x640 (no detections), 51.8ms\n",
      "Speed: 1.1ms preprocess, 51.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.002729136191917\n",
      "\n",
      "0: 352x640 (no detections), 57.2ms\n",
      "Speed: 1.3ms preprocess, 57.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.007354162814975\n",
      "\n",
      "0: 352x640 (no detections), 54.9ms\n",
      "Speed: 1.2ms preprocess, 54.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.01331059225005\n",
      "\n",
      "0: 352x640 (no detections), 50.5ms\n",
      "Speed: 1.2ms preprocess, 50.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.026025835514874\n",
      "\n",
      "0: 352x640 (no detections), 56.9ms\n",
      "Speed: 1.2ms preprocess, 56.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.029254866504312\n",
      "\n",
      "0: 352x640 (no detections), 53.7ms\n",
      "Speed: 1.5ms preprocess, 53.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.035811397712445\n",
      "\n",
      "0: 352x640 (no detections), 59.5ms\n",
      "Speed: 1.1ms preprocess, 59.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.037381790915001\n",
      "\n",
      "0: 352x640 (no detections), 49.5ms\n",
      "Speed: 1.4ms preprocess, 49.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.05015241784313\n",
      "\n",
      "0: 352x640 (no detections), 52.5ms\n",
      "Speed: 1.3ms preprocess, 52.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.058802270502257\n",
      "\n",
      "0: 352x640 (no detections), 55.3ms\n",
      "Speed: 1.3ms preprocess, 55.3ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.062935841344387\n",
      "\n",
      "0: 352x640 (no detections), 50.4ms\n",
      "Speed: 1.2ms preprocess, 50.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.072959334034824\n",
      "\n",
      "0: 352x640 (no detections), 52.4ms\n",
      "Speed: 1.2ms preprocess, 52.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.080605666086871\n",
      "\n",
      "0: 352x640 (no detections), 46.8ms\n",
      "Speed: 1.1ms preprocess, 46.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.094989470493624\n",
      "\n",
      "0: 352x640 (no detections), 51.5ms\n",
      "Speed: 1.2ms preprocess, 51.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.100505239720384\n",
      "\n",
      "0: 352x640 (no detections), 53.4ms\n",
      "Speed: 1.4ms preprocess, 53.4ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.101513831933316\n",
      "\n",
      "0: 352x640 (no detections), 51.2ms\n",
      "Speed: 1.0ms preprocess, 51.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.10988500658537\n",
      "\n",
      "0: 352x640 (no detections), 44.4ms\n",
      "Speed: 1.0ms preprocess, 44.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.125826534871734\n",
      "\n",
      "0: 352x640 (no detections), 46.7ms\n",
      "Speed: 1.5ms preprocess, 46.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "10.138254765654512\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compteur \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     35\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 36\u001b[0m     result \u001b[38;5;241m=\u001b[39m model(frame)\n\u001b[1;32m     37\u001b[0m     result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(result\u001b[38;5;241m.\u001b[39mboxes)):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:169\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    149\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    151\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/model.py:429\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:204\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:283\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 283\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/engine/predictor.py:140\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    136\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    139\u001b[0m )\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/autobackend.py:384\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    381\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39membed)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:83\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:101\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/tasks.py:122\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 122\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    123\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:225\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 225\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:225\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 225\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/block.py:335\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"'forward()' applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import  numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "model = YOLO('player_detection on minecraft 04 (gpu).pt')\n",
    "#Desktop/video_dataset/minecraft-pvp-montage-a-l-ancienne-planetpvp-2.mp4\n",
    "cap = cv2.VideoCapture('hypixel-pvp-montage.mp4')\n",
    "\n",
    "# create a dictionary of all trackers in OpenCV that can be used for tracking\n",
    "OPENCV_OBJECT_TRACKERS = {\n",
    "\t\"csrt\": cv2.legacy.TrackerCSRT_create, # peu grossir/rapetir est 7/10\n",
    "\t\"kcf\": cv2.legacy.TrackerKCF_create, # 5/10 \n",
    "\t\"boosting\": cv2.legacy.TrackerBoosting_create,# 3/10  n'arete pas de tracker\n",
    "\t\"mil\": cv2.legacy.TrackerMIL_create,# 3/10  n'arete pas de tracker\n",
    "\t\"tld\": cv2.legacy.TrackerTLD_create,# 4/10  n'arete pas de tracker mais retrouve des chose semblable\n",
    "\t\"medianflow\": cv2.legacy.TrackerMedianFlow_create,# 4/10  n'arete pas de tracker mais retrouve des chose semblable\n",
    "\t\"mosse\": cv2.legacy.TrackerMOSSE_create# 4/10  n'arete pas de tracker \n",
    "}\n",
    "\n",
    "\n",
    "# Create MultiTracker object\n",
    "#trackers = cv2.legacy.MultiTracker_create()\n",
    "compteur = 0\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    frame = cap.read()[1]\n",
    "\n",
    "    if frame is None:\n",
    "        break\n",
    "    frame = cv2.resize(frame,(1090,600))\n",
    "    \n",
    "    \n",
    "    if compteur % 1 == 0:\n",
    "        boxes = []\n",
    "        result = model(frame)\n",
    "        result = result[0]\n",
    "        for idx in range(len(result.boxes)):\n",
    "            box = result.boxes[idx]\n",
    "            pos = box.xyxy[0]\n",
    "\n",
    "            x1 = int(pos[0].item())\n",
    "            y1 = int(pos[1].item())\n",
    "\n",
    "            x2 = int(pos[2].item())\n",
    "            y2 = int(pos[3].item())\n",
    "\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "\n",
    "            bound_box = (x1, y1, w, h)\n",
    "            #tracker = OPENCV_OBJECT_TRACKERS['csrt']()\n",
    "            #trackers.add(tracker, frame, bound_box)\n",
    "\n",
    "            if boxes == ():\n",
    "                boxes = []\n",
    "                boxes.append([x1, y1, w, h])\n",
    "                print(\"1 :\", boxes)\n",
    "            else:\n",
    "                try:\n",
    "                    boxes = boxes.tolist()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                boxes.append([x1, y1, w, h])\n",
    "                print(\"2 \", boxes)\n",
    "        \n",
    "    \n",
    "        \n",
    "    for i,box in enumerate(boxes):\n",
    "        #print(\"box : \", box)\n",
    "        (x, y, w, h) = [int(v) for v in box]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow('Frame', frame)\n",
    "    k = cv2.waitKey(30)\n",
    "\n",
    "    if k == ord(\"s\"): \n",
    "        # select the bounding box of the object we want to track (make\n",
    "        # sure you press ENTER or SPACE after selecting the ROI)\n",
    "        roi = cv2.selectROI(\"Frame\", frame, fromCenter=False,\n",
    "                            showCrosshair=True)\n",
    "        # create a new object tracker for the bounding box and add it\n",
    "        # to our multi-object tracker\n",
    "        #tracker = OPENCV_OBJECT_TRACKERS['csrt']()\n",
    "        #trackers.add(tracker, frame, roi)\n",
    "    \n",
    "    compteur += 1 \n",
    "    fps = compteur / (time.time() - start_time)\n",
    "    print(fps)\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%1 = 10 fps\n",
    "%2 = 14,8 fps\n",
    "\n",
    "% 5 with traking = 16\n",
    "% 7 with traking = 16,9\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c7b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 352x640 (no detections), 54.7ms\n",
      "Speed: 1.6ms preprocess, 54.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "30.558775600408975\n",
      "29.88554578036459\n",
      "29.480907593296287\n",
      "29.147802452358157\n",
      "28.895883840628084\n",
      "\n",
      "0: 352x640 (no detections), 58.4ms\n",
      "Speed: 1.4ms preprocess, 58.4ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "24.719144563350337\n",
      "24.884283219652847\n",
      "25.02565721896712\n",
      "25.10724878857625\n",
      "25.20072195976989\n",
      "\n",
      "0: 352x640 (no detections), 56.9ms\n",
      "Speed: 1.2ms preprocess, 56.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "23.105293890816945\n",
      "23.296291153454966\n",
      "23.474947039678963\n",
      "23.632488446464667\n",
      "23.776182232304922\n",
      "\n",
      "0: 352x640 (no detections), 57.9ms\n",
      "Speed: 1.2ms preprocess, 57.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "22.360620296337856\n",
      "22.526674577985034\n",
      "22.68935578913896\n",
      "22.837262494915326\n",
      "22.976459655406572\n",
      "\n",
      "0: 352x640 (no detections), 53.9ms\n",
      "Speed: 1.5ms preprocess, 53.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "21.985380027946448\n",
      "22.129278788029193\n",
      "22.268792482646667\n",
      "22.406539068232146\n",
      "22.529321177477254\n",
      "\n",
      "0: 352x640 (no detections), 50.9ms\n",
      "Speed: 1.2ms preprocess, 50.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "21.784423324434602\n",
      "21.909796776746948\n",
      "22.031386584911434\n",
      "22.14947466451871\n",
      "22.266217865801423\n",
      "\n",
      "0: 352x640 (no detections), 81.8ms\n",
      "Speed: 1.1ms preprocess, 81.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "21.25827942677312\n",
      "21.3802261363544\n",
      "21.495414153102523\n",
      "21.594755724441075\n",
      "21.672523180400862\n",
      "\n",
      "0: 352x640 (no detections), 57.8ms\n",
      "Speed: 1.5ms preprocess, 57.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "21.086609169926838\n",
      "21.19173237326158\n",
      "21.295878516186008\n",
      "21.3983440141829\n",
      "21.495327698844417\n",
      "\n",
      "0: 352x640 (no detections), 58.6ms\n",
      "Speed: 1.2ms preprocess, 58.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.981333334493296\n",
      "21.074833767536365\n",
      "21.171872071900403\n",
      "21.270073699780305\n",
      "21.363795605774552\n",
      "\n",
      "0: 352x640 (no detections), 54.5ms\n",
      "Speed: 1.1ms preprocess, 54.5ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.93069283679599\n",
      "21.021685017188133\n",
      "21.108632516477076\n",
      "21.1917275578874\n",
      "21.274931468089825\n",
      "\n",
      "0: 352x640 (no detections), 55.8ms\n",
      "Speed: 1.3ms preprocess, 55.8ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.883080155541283\n",
      "20.96313474610156\n",
      "21.041977974870683\n",
      "21.120298181844873\n",
      "21.197324499228028\n",
      "\n",
      "0: 352x640 (no detections), 54.0ms\n",
      "Speed: 1.6ms preprocess, 54.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.85080239951761\n",
      "20.927028125606824\n",
      "21.000302080685028\n",
      "21.072872284095766\n",
      "21.144173562461727\n",
      "\n",
      "0: 352x640 (no detections), 52.2ms\n",
      "Speed: 1.1ms preprocess, 52.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.84632361329967\n",
      "20.914460980259975\n",
      "20.98396225526665\n",
      "21.049099659920497\n",
      "21.11369358008441\n",
      "\n",
      "0: 352x640 (no detections), 56.9ms\n",
      "Speed: 1.2ms preprocess, 56.9ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.80958043126923\n",
      "20.87271667642142\n",
      "20.936444285806108\n",
      "20.99662515396788\n",
      "21.057774046345433\n",
      "\n",
      "0: 352x640 (no detections), 48.2ms\n",
      "Speed: 1.2ms preprocess, 48.2ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.8189907599337\n",
      "20.877703193739407\n",
      "20.9355055623544\n",
      "20.99206432219076\n",
      "21.048139173116237\n",
      "\n",
      "0: 352x640 (no detections), 53.9ms\n",
      "Speed: 1.2ms preprocess, 53.9ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.79421084475064\n",
      "20.850480418429502\n",
      "20.905432387025666\n",
      "20.960389783496513\n",
      "21.01284361103554\n",
      "\n",
      "0: 352x640 (no detections), 51.6ms\n",
      "Speed: 1.2ms preprocess, 51.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.78804136283189\n",
      "20.842956094896724\n",
      "20.89780299991779\n",
      "20.951848803527835\n",
      "21.002869115807467\n",
      "\n",
      "0: 352x640 (no detections), 55.5ms\n",
      "Speed: 1.2ms preprocess, 55.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.772228726667603\n",
      "20.824213587385493\n",
      "20.874309686519535\n",
      "20.92540619262975\n",
      "20.974682761374282\n",
      "\n",
      "0: 352x640 (no detections), 50.6ms\n",
      "Speed: 1.2ms preprocess, 50.6ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.77749820671974\n",
      "20.825349661750415\n",
      "20.874225872216513\n",
      "20.92280157354814\n",
      "20.969046700941625\n",
      "\n",
      "0: 352x640 (no detections), 56.1ms\n",
      "Speed: 1.2ms preprocess, 56.1ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.749905869201612\n",
      "20.795089971466595\n",
      "20.840823754487996\n",
      "20.88647889370055\n",
      "20.931420382219187\n",
      "\n",
      "0: 352x640 (no detections), 56.0ms\n",
      "Speed: 1.2ms preprocess, 56.0ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.731610285316197\n",
      "20.77788564069648\n",
      "20.82107520047391\n",
      "20.862537168223977\n",
      "20.905250356370306\n",
      "\n",
      "0: 352x640 (no detections), 50.7ms\n",
      "Speed: 1.3ms preprocess, 50.7ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.736485942975523\n",
      "20.761949438491513\n",
      "20.798776041675808\n",
      "20.839427279367797\n",
      "20.88032563172357\n",
      "\n",
      "0: 352x640 (no detections), 57.5ms\n",
      "Speed: 1.3ms preprocess, 57.5ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.692887592759067\n",
      "20.733174439159942\n",
      "20.771597772204768\n",
      "20.8110579986881\n",
      "20.850567594929117\n",
      "\n",
      "0: 352x640 1 player, 50.3ms\n",
      "Speed: 1.2ms preprocess, 50.3ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.601067684983203\n",
      "20.579148177321724\n",
      "20.560155583053263\n",
      "20.591346454019078\n",
      "20.576480383705395\n",
      "\n",
      "0: 352x640 (no detections), 53.1ms\n",
      "Speed: 1.4ms preprocess, 53.1ms inference, 0.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.425151256498054\n",
      "20.463282947985405\n",
      "20.501304952603103\n",
      "20.538128083028717\n",
      "20.574135324092534\n",
      "\n",
      "0: 352x640 1 player, 54.2ms\n",
      "Speed: 2.0ms preprocess, 54.2ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.364319721560218\n",
      "20.350321053910907\n",
      "20.33254233926221\n",
      "20.315924362756952\n",
      "20.345515611973717\n",
      "\n",
      "0: 352x640 1 player, 60.4ms\n",
      "Speed: 1.4ms preprocess, 60.4ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "20.149360174969495\n",
      "20.138911964666637\n",
      "20.125175434762678\n",
      "20.11076896075434\n",
      "20.09941774924196\n",
      "\n",
      "0: 352x640 1 player, 51.9ms\n",
      "Speed: 1.7ms preprocess, 51.9ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "19.938087224705694\n",
      "19.92808477926458\n",
      "19.9171168853258\n",
      "19.907986949679824\n",
      "19.898629795636747\n",
      "\n",
      "0: 352x640 1 player, 49.9ms\n",
      "Speed: 1.6ms preprocess, 49.9ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "19.754000448068634\n",
      "19.7441586289052\n",
      "19.735351582401293\n",
      "19.727399626665377\n",
      "19.71884612084034\n",
      "\n",
      "0: 352x640 1 player, 57.3ms\n",
      "Speed: 1.6ms preprocess, 57.3ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "19.562707112864736\n",
      "19.554826829291933\n",
      "19.547030175929162\n",
      "19.538198399751334\n",
      "19.529893799348407\n",
      "\n",
      "0: 352x640 1 player, 52.5ms\n",
      "Speed: 1.8ms preprocess, 52.5ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "19.39495680051107\n",
      "19.38694017569096\n",
      "19.381440267015225\n",
      "19.375388306387542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (x, y), (x \u001b[38;5;241m+\u001b[39m w, y \u001b[38;5;241m+\u001b[39m h), (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     76\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[0;32m---> 77\u001b[0m k \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m): \n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# select the bounding box of the object we want to track (make\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# sure you press ENTER or SPACE after selecting the ROI)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     roi \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mselectROI(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame, fromCenter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m                         showCrosshair\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###################### with tracking #######################\n",
    "\n",
    "import cv2\n",
    "import  numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "model = YOLO('player_detection on minecraft 04 (gpu).pt')\n",
    "#Desktop/video_dataset/minecraft-pvp-montage-a-l-ancienne-planetpvp-2.mp4\n",
    "cap = cv2.VideoCapture('hypixel-pvp-montage.mp4')\n",
    "\n",
    "# create a dictionary of all trackers in OpenCV that can be used for tracking\n",
    "OPENCV_OBJECT_TRACKERS = {\n",
    "\t\"csrt\": cv2.legacy.TrackerCSRT_create, # peu grossir/rapetir est 7/10\n",
    "\t\"kcf\": cv2.legacy.TrackerKCF_create, # 5/10 \n",
    "\t\"boosting\": cv2.legacy.TrackerBoosting_create,# 3/10  n'arete pas de tracker\n",
    "\t\"mil\": cv2.legacy.TrackerMIL_create,# 3/10  n'arete pas de tracker\n",
    "\t\"tld\": cv2.legacy.TrackerTLD_create,# 4/10  n'arete pas de tracker mais retrouve des chose semblable\n",
    "\t\"medianflow\": cv2.legacy.TrackerMedianFlow_create,# 4/10  n'arete pas de tracker mais retrouve des chose semblable\n",
    "\t\"mosse\": cv2.legacy.TrackerMOSSE_create# 4/10  n'arete pas de tracker \n",
    "}\n",
    "\n",
    "\n",
    "trackers = cv2.legacy.MultiTracker_create()\n",
    "compteur = 5\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    frame = cap.read()[1]\n",
    "\n",
    "    if frame is None:\n",
    "        break\n",
    "    frame = cv2.resize(frame,(1090,600))\n",
    "    \n",
    "    if compteur % 5 == 0:\n",
    "        trackers = cv2.legacy.MultiTracker_create()\n",
    "        boxes = []\n",
    "        result = model(frame)\n",
    "        result = result[0]\n",
    "        for idx in range(len(result.boxes)):\n",
    "            box = result.boxes[idx]\n",
    "            pos = box.xyxy[0]\n",
    "\n",
    "            x1 = int(pos[0].item())\n",
    "            y1 = int(pos[1].item())\n",
    "\n",
    "            x2 = int(pos[2].item())\n",
    "            y2 = int(pos[3].item())\n",
    "\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "\n",
    "            bound_box = (x1, y1, w, h)\n",
    "            tracker = OPENCV_OBJECT_TRACKERS['csrt']()\n",
    "            trackers.add(tracker, frame, bound_box)\n",
    "\n",
    "            if boxes == ():\n",
    "                boxes = []\n",
    "                boxes.append([x1, y1, w, h])\n",
    "            else:\n",
    "                try:\n",
    "                    boxes = boxes.tolist()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                boxes.append([x1, y1, w, h])\n",
    "    else:\n",
    "        (success, boxes) = trackers.update(frame)\n",
    "        \n",
    "    \n",
    "        \n",
    "    for i,box in enumerate(boxes):\n",
    "        #print(\"box : \", box)\n",
    "        (x, y, w, h) = [int(v) for v in box]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow('Frame', frame)\n",
    "    k = cv2.waitKey(30)\n",
    "\n",
    "    if k == ord(\"s\"): \n",
    "        # select the bounding box of the object we want to track (make\n",
    "        # sure you press ENTER or SPACE after selecting the ROI)\n",
    "        roi = cv2.selectROI(\"Frame\", frame, fromCenter=False,\n",
    "                            showCrosshair=True)\n",
    "        # create a new object tracker for the bounding box and add it\n",
    "        # to our multi-object tracker\n",
    "        tracker = OPENCV_OBJECT_TRACKERS['csrt']()\n",
    "        trackers.add(tracker, frame, roi)\n",
    "    \n",
    "    compteur += 1 \n",
    "    fps = compteur / (time.time() - start_time)\n",
    "    print(fps)\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# fps au bout de 1min05 sec = 16\n",
    "\"\"\"\n",
    "0: 352x640 1 player, 56.7ms\n",
    "Speed: 1.3ms preprocess, 56.7ms inference, 0.3ms postprocess per image at shape (1, 3, 352, 640)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a736bc0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/arthur/Documents/Yolo/arthur/player detection.ipynb Cellule 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arthur/Documents/Yolo/arthur/player%20detection.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m############ optimize ##########\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arthur/Documents/Yolo/arthur/player%20detection.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arthur/Documents/Yolo/arthur/player%20detection.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m  \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arthur/Documents/Yolo/arthur/player%20detection.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39multralytics\u001b[39;00m \u001b[39mimport\u001b[39;00m YOLO\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "############ optimize ##########\n",
    "\n",
    "import cv2\n",
    "import  numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "model = YOLO('player_detection on minecraft 04 (gpu)_openvino_model')\n",
    "#Desktop/video_dataset/minecraft-pvp-montage-a-l-ancienne-planetpvp-2.mp4\n",
    "cap = cv2.VideoCapture('the-most-intense-minecraft-pvp-battle.mp4')\n",
    "\n",
    "# create a dictionary of all trackers in OpenCV that can be used for tracking\n",
    "OPENCV_OBJECT_TRACKERS = {\n",
    "\t\"csrt\": cv2.legacy.TrackerCSRT_create, # peu grossir/rapetir est 7/10\n",
    "\t\"kcf\": cv2.legacy.TrackerKCF_create, # 5/10 \n",
    "\t\"boosting\": cv2.legacy.TrackerBoosting_create,# 3/10  n'arete pas de tracker\n",
    "\t\"mil\": cv2.legacy.TrackerMIL_create,# 3/10  n'arete pas de tracker\n",
    "\t\"tld\": cv2.legacy.TrackerTLD_create,# 4/10  n'arete pas de tracker mais retrouve des chose semblable\n",
    "\t\"medianflow\": cv2.legacy.TrackerMedianFlow_create,# 4/10  n'arete pas de tracker mais retrouve des chose semblable\n",
    "\t\"mosse\": cv2.legacy.TrackerMOSSE_create# 4/10  n'arete pas de tracker \n",
    "}\n",
    "\n",
    "\n",
    "trackers = cv2.legacy.MultiTracker_create()\n",
    "compteur = 5\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    frame = cap.read()[1]\n",
    "\n",
    "    if frame is None:\n",
    "        break\n",
    "    frame = cv2.resize(frame,(1090,600))\n",
    "    \n",
    "    if compteur % 5 == 0:\n",
    "        trackers = cv2.legacy.MultiTracker_create()\n",
    "        boxes = []\n",
    "        result = model(frame)\n",
    "        result = result[0]\n",
    "        for idx in range(len(result.boxes)):\n",
    "            box = result.boxes[idx]\n",
    "            pos = box.xyxy[0]\n",
    "\n",
    "            x1 = int(pos[0].item())\n",
    "            y1 = int(pos[1].item())\n",
    "\n",
    "            x2 = int(pos[2].item())\n",
    "            y2 = int(pos[3].item())\n",
    "\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "\n",
    "            bound_box = (x1, y1, w, h)\n",
    "            tracker = OPENCV_OBJECT_TRACKERS['csrt']()\n",
    "            trackers.add(tracker, frame, bound_box)\n",
    "\n",
    "            if boxes == ():\n",
    "                boxes = []\n",
    "                boxes.append([x1, y1, w, h])\n",
    "            else:\n",
    "                try:\n",
    "                    boxes = boxes.tolist()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                boxes.append([x1, y1, w, h])\n",
    "    else:\n",
    "        (success, boxes) = trackers.update(frame)\n",
    "        \n",
    "    \n",
    "        \n",
    "    for i,box in enumerate(boxes):\n",
    "        #print(\"box : \", box)\n",
    "        (x, y, w, h) = [int(v) for v in box]\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow('Frame', frame)\n",
    "    k = cv2.waitKey(30)\n",
    "\n",
    "    if k == ord(\"s\"): \n",
    "        # select the bounding box of the object we want to track (make\n",
    "        # sure you press ENTER or SPACE after selecting the ROI)\n",
    "        roi = cv2.selectROI(\"Frame\", frame, fromCenter=False,\n",
    "                            showCrosshair=True)\n",
    "        # create a new object tracker for the bounding box and add it\n",
    "        # to our multi-object tracker\n",
    "        tracker = OPENCV_OBJECT_TRACKERS['csrt']()\n",
    "        trackers.add(tracker, frame, roi)\n",
    "    \n",
    "    compteur += 1 \n",
    "    fps = compteur / (time.time() - start_time)\n",
    "    print(fps)\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# fps au bout de 1min05 = 16,17\n",
    "\"\"\"\n",
    "0: 640x640 1 player, 47.6ms\n",
    "Speed: 1.4ms preprocess, 47.6ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 640)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e0d2c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file exists at /Users/arthur/Library/Preferences/pypoetry, reusing this directory.\n",
      "\n",
      "Consider moving TOML configuration files to /Users/arthur/Library/Application Support/pypoetry, as support for the legacy directory will be removed in an upcoming release.\n",
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "   \u001b[36mopencv-python\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "!poetry add opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b7e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
